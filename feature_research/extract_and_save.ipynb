{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# abilities = pd.read_csv('./sberbank_data/final/dota2_abilities.csv')\n",
    "# items = pd.read_csv('./sberbank_data/data_2019/dota2_items.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# LOAD DATA\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess  skill_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "skill_train = pd.read_csv('./sberbank_data/final/academy2019_final_train.csv', index_col='id')\n",
    "\n",
    "# test\n",
    "skill_test = pd.read_csv('./sberbank_data/final/academy2019_final_test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "# delete 'other' from 'winner_team'\n",
    "skill_train = skill_train[skill_train['winner_team'] != 'other']\n",
    "\n",
    "# test\n",
    "skill_test = skill_test[skill_test['winner_team'] != 'other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split on data and targets in train data set\n",
    "data_train = skill_train.copy()\n",
    "# test\n",
    "data_test = skill_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get dummies on 'hero_id', 'player_team' and 'winner_team'\n",
    "# train\n",
    "player_team = pd.get_dummies(data_train['player_team'], prefix='player_team')\n",
    "winner_team = pd.get_dummies(data_train['winner_team'], prefix='winner_team')\n",
    "# test\n",
    "player_team_test = pd.get_dummies(data_test['player_team'], prefix='player_team')\n",
    "winner_team_test = pd.get_dummies(data_test['winner_team'], prefix='winner_team')\n",
    "\n",
    "# drop colums\n",
    "data_train.drop(['player_team'], inplace=True, axis=1)\n",
    "data_train.drop(['winner_team'], inplace=True, axis=1)\n",
    "# test\n",
    "data_test.drop(['player_team'], inplace=True, axis=1)\n",
    "data_test.drop(['winner_team'], inplace=True, axis=1)\n",
    "\n",
    "# concatenate them to dataframe\n",
    "data_train = pd.concat([data_train, player_team, winner_team], axis=1)\n",
    "#test\n",
    "data_test = pd.concat([data_test, player_team_test, winner_team_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess heros dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroes = pd.read_csv('./sberbank_data/data_2019/dota2_heroes.csv', index_col='hero_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### roles ====> one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroes['roles'] = heroes['roles'].apply(lambda x: x.replace(\" \", '').strip('[]').replace(\"'\", '').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "heroes = heroes.join(pd.DataFrame(mlb.fit_transform(heroes.pop('roles')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=heroes.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attack_type, cm_enabled ====> one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get dummies\n",
    "attack_type_dummies = pd.get_dummies(heroes['attack_type'], prefix='att_type_')\n",
    "captain_mode_dummies = pd.get_dummies(heroes['cm_enabled'], prefix='cm_')\n",
    "prim_attr_dummies = pd.get_dummies(heroes['primary_attr'], prefix='pr_attr_')\n",
    "\n",
    "# drop colums\n",
    "heroes.drop(['attack_type'], inplace=True, axis=1)\n",
    "heroes.drop(['cm_enabled'], inplace=True, axis=1)\n",
    "heroes.drop(['primary_attr'], inplace=True, axis=1)\n",
    "\n",
    "# concatenate them to dataframe\n",
    "heroes = pd.concat([heroes, \n",
    "                    attack_type_dummies, \n",
    "                    captain_mode_dummies, \n",
    "                    prim_attr_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join heroes to data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heroes.drop(['name', 'localized_name'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train.join(heroes, on='hero_id')\n",
    "data_test = data_test.join(heroes, on='hero_id')\n",
    "\n",
    "# apply one hot for 'hero_id'\n",
    "# train\n",
    "hero_dummies = pd.get_dummies(data_train['hero_id'], prefix='hero_id')\n",
    "data_train.drop(['hero_id'], inplace=True, axis=1)\n",
    "data_train = pd.concat([data_train, hero_dummies], axis=1)\n",
    "# test\n",
    "hero_dummies_test = pd.get_dummies(data_test['hero_id'], prefix='hero_id')\n",
    "data_test.drop(['hero_id'], inplace=True, axis=1)\n",
    "data_test = pd.concat([data_test, hero_dummies_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40395, 211), (15835, 210))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse JSON (extract extra features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timur/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "from source.functions_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_train = './sberbank_data/final/academy2019_final_train.jsonlines'\n",
    "json_test = './sberbank_data/final/academy2019_final_test.jsonlines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_data(path_to_json):\n",
    "    buff = []\n",
    "    ids = []\n",
    "\n",
    "    data_gold = []\n",
    "    data_items = []\n",
    "    data_damage = []\n",
    "    data_abilities = []\n",
    "    data_items_count = []\n",
    "    data_targets_count = []\n",
    "    data_gold_deviation = []\n",
    "    data_lvls = []\n",
    "\n",
    "    batch_size = 10000\n",
    "\n",
    "    with open(path_to_json) as inp:\n",
    "        total = 40395 if 'train' in path_to_json else 15835\n",
    "        for i, line in tqdm(enumerate(inp), total=total):\n",
    "            record = json.loads(line)\n",
    "            buff.append(record)\n",
    "            ids.append(record['id'])\n",
    "\n",
    "            if i + 1 == batch_size:\n",
    "                # extract features\n",
    "                data_gold.append(get_gold_features(buff))\n",
    "                data_items.append(get_items_features(buff))\n",
    "                data_damage.append(get_damage_features(buff))\n",
    "                data_lvls.append(get_lvl_times(buff))\n",
    "                \n",
    "                data_abilities.append(get_abilities(buff))\n",
    "                data_items_count.append(get_items_value(buff))\n",
    "                data_targets_count.append(get_targets_value(buff))\n",
    "                data_gold_deviation.append(gold_deviation(buff))\n",
    "\n",
    "                # free buff\n",
    "                buff = []\n",
    "\n",
    "        # process last batch\n",
    "        data_gold.append(get_gold_features(buff))\n",
    "        data_items.append(get_items_features(buff))\n",
    "        data_damage.append(get_damage_features(buff))\n",
    "        data_lvls.append(get_lvl_times(buff))\n",
    "        \n",
    "        data_abilities.append(get_abilities(buff))\n",
    "        data_items_count.append(get_items_value(buff))\n",
    "        data_targets_count.append(get_targets_value(buff))\n",
    "        data_gold_deviation.append(gold_deviation(buff))\n",
    "\n",
    "        # free buff\n",
    "        buff = []\n",
    "        \n",
    "    #========================\n",
    "    # convert to numpy arrays\n",
    "    data_gold = np.concatenate(data_gold)\n",
    "    data_items = np.concatenate(data_items)\n",
    "    data_damage = np.concatenate(data_damage)\n",
    "    data_lvls = np.concatenate(data_lvls)\n",
    "    \n",
    "    data_abilities = np.concatenate(data_abilities)\n",
    "    data_items_count = np.concatenate(data_items_count)\n",
    "    data_targets_count = np.concatenate(data_targets_count)\n",
    "    data_gold_deviation = np.concatenate(data_gold_deviation)\n",
    "    \n",
    "    # merge all data\n",
    "    extra_data = np.concatenate((data_gold,\n",
    "                                 data_items, \n",
    "                                 data_damage, \n",
    "                                 data_lvls,\n",
    "                                 \n",
    "                                 data_abilities,\n",
    "                                 data_items_count, \n",
    "                                 data_targets_count, \n",
    "                                 data_gold_deviation\n",
    "                                ), axis=1)\n",
    "    \n",
    "    return extra_data, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8cfd31e24a460997497dd90bc37fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87adaaae3e148a8ac5167c59e374f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extra_data, ids = extract_data(json_train)\n",
    "extra_data_test, ids_test = extract_data(json_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  184.03 Mb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"% 8.2f Mb\" % (sys.getsizeof(extra_data) / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40403, 597), (15836, 597))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_data.shape, extra_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = columns_gold + columns_items + columns_damage + columns_lvl\n",
    "columns += columns_abililies + columns_items_value + columns_tar_value + columns_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extra_data = pd.DataFrame(data=extra_data, \n",
    "                          index=ids, \n",
    "                          columns=columns)\n",
    "\n",
    "extra_data_test = pd.DataFrame(data=extra_data_test, \n",
    "                          index=ids_test, \n",
    "                          columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train.join(extra_data)\n",
    "data_test = data_test.join(extra_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_name = 'train_extracted_final'\n",
    "test_name = 'test_extracted_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40395, 808), (15835, 807))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.to_csv('./sberbank_data/data_2019/' + train_name + '.csv')\n",
    "data_test.to_csv('./sberbank_data/data_2019/' + test_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
